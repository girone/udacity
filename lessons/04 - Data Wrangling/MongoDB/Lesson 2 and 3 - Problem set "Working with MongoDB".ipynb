{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set: Working with MongoDB\n",
    "\n",
    "## Quiz 1: Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with another type of infobox data, audit it,\n",
    "clean it, come up with a data model, insert it into MongoDB and then run some\n",
    "queries against your database. The set contains data about Arachnid class\n",
    "animals.\n",
    "\n",
    "Your task in this exercise is to parse the file, process only the fields that\n",
    "are listed in the FIELDS dictionary as keys, and return a list of dictionaries\n",
    "of cleaned values. \n",
    "\n",
    "The following things should be done:\n",
    "- keys of the dictionary changed according to the mapping in FIELDS dictionary\n",
    "- trim out redundant description in parenthesis from the 'rdf-schema#label'\n",
    "  field, like \"(spider)\"\n",
    "- if 'name' is \"NULL\" or contains non-alphanumeric characters, set it to the\n",
    "  same value as 'label'.\n",
    "- if a value of a field is \"NULL\", convert it to None\n",
    "- if there is a value in 'synonym', it should be converted to an array (list)\n",
    "  by stripping the \"{}\" characters and splitting the string on \"|\". Rest of the\n",
    "  cleanup is up to you, e.g. removing \"*\" prefixes etc. If there is a singular\n",
    "  synonym, the value should still be formatted in a list.\n",
    "- strip leading and ending whitespace from all fields, if there is any\n",
    "- the output structure should be as follows:\n",
    "\n",
    "[ { 'label': 'Argiope',\n",
    "    'uri': 'http://dbpedia.org/resource/Argiope_(spider)',\n",
    "    'description': 'The genus Argiope includes rather large and spectacular spiders that often ...',\n",
    "    'name': 'Argiope',\n",
    "    'synonym': [\"One\", \"Two\"],\n",
    "    'classification': {\n",
    "                      'family': 'Orb-weaver spider',\n",
    "                      'class': 'Arachnid',\n",
    "                      'phylum': 'Arthropod',\n",
    "                      'order': 'Spider',\n",
    "                      'kingdom': 'Animal',\n",
    "                      'genus': None\n",
    "                      }\n",
    "  },\n",
    "  { 'label': ... , }, ...\n",
    "]\n",
    "\n",
    "  * Note that the value associated with the classification key is a dictionary\n",
    "    with taxonomic labels.\n",
    "\"\"\"\n",
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "DATAFILE = 'arachnid.csv'\n",
    "FIELDS ={'rdf-schema#label': 'label',\n",
    "         'URI': 'uri',\n",
    "         'rdf-schema#comment': 'description',\n",
    "         'synonym': 'synonym',\n",
    "         'name': 'name',\n",
    "         'family_label': 'family',\n",
    "         'class_label': 'class',\n",
    "         'phylum_label': 'phylum',\n",
    "         'order_label': 'order',\n",
    "         'kingdom_label': 'kingdom',\n",
    "         'genus_label': 'genus'}\n",
    "\n",
    "\n",
    "PATTERN_DISTINCTION = re.compile(r\" \\(.*\\)\")\n",
    "PATTERN_ALNUM = re.compile(r\"[a-z0-9]+\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def process_file(filename, fields):\n",
    "    process_fields = fields.keys()\n",
    "    data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i in range(3):\n",
    "            l = next(reader)\n",
    "\n",
    "        for line in reader:\n",
    "            entry = {}\n",
    "            taxonomy_labels = {}\n",
    "            for key, new_key in FIELDS.items():\n",
    "                value = line[key]\n",
    "                if new_key == \"label\":\n",
    "                    value = PATTERN_DISTINCTION.sub(\"\", value)\n",
    "                if value == \"NULL\":\n",
    "                    value = None\n",
    "                    \n",
    "                if value and new_key == \"synonym\":\n",
    "                    value = parse_array(value)\n",
    "                    \n",
    "                if key.endswith(\"_label\"):\n",
    "                    taxonomy_labels[new_key] = value\n",
    "                else:\n",
    "                    entry[new_key] = value\n",
    "            entry[\"classification\"] = taxonomy_labels\n",
    "            name = entry[\"name\"]\n",
    "            if not name or not PATTERN_ALNUM.match(name):\n",
    "                entry[\"name\"] = entry[\"label\"]\n",
    "                \n",
    "                    \n",
    "            #print(entry)\n",
    "            #line = parse_array(line)\n",
    "            data.append(entry)\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_array(v):\n",
    "    if (v[0] == \"{\") and (v[-1] == \"}\"):\n",
    "        v = v.lstrip(\"{\")\n",
    "        v = v.rstrip(\"}\")\n",
    "        v_array = v.split(\"|\")\n",
    "        v_array = [i.strip() for i in v_array]\n",
    "        return v_array\n",
    "    return [v]\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = process_file(DATAFILE, FIELDS)\n",
    "    print(\"Your first entry:\")\n",
    "    pprint.pprint(data[0])\n",
    "    first_entry = {\n",
    "        \"synonym\": None, \n",
    "        \"name\": \"Argiope\", \n",
    "        \"classification\": {\n",
    "            \"kingdom\": \"Animal\", \n",
    "            \"family\": \"Orb-weaver spider\", \n",
    "            \"order\": \"Spider\", \n",
    "            \"phylum\": \"Arthropod\", \n",
    "            \"genus\": None, \n",
    "            \"class\": \"Arachnid\"\n",
    "        }, \n",
    "        \"uri\": \"http://dbpedia.org/resource/Argiope_(spider)\", \n",
    "        \"label\": \"Argiope\", \n",
    "        \"description\": \"The genus Argiope includes rather large and spectacular spiders that often have a strikingly coloured abdomen. These spiders are distributed throughout the world. Most countries in tropical or temperate climates host one or more species that are similar in appearance. The etymology of the name is from a Greek name meaning silver-faced.\"\n",
    "    }\n",
    "    pprint.pprint(first_entry)\n",
    "\n",
    "    assert len(data) == 76\n",
    "    assert data[0] == first_entry\n",
    "    assert data[17][\"name\"] == \"Ogdenia\"\n",
    "    assert data[48][\"label\"] == \"Hydrachnidiae\"\n",
    "    assert data[14][\"synonym\"] == [\"Cyrene Peckham & Peckham\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 2: Inserting into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete the insert_data function to insert the data into MongoDB.\n",
    "\"\"\"\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "def insert_data(data, db):\n",
    "    # Insert the data into a collection 'arachnid'\n",
    "    db.arachnid.insert_many(data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client.examples\n",
    "\n",
    "    with open('arachnid.json') as f:\n",
    "        data = json.loads(f.read())\n",
    "        insert_data(data, db)\n",
    "        print(db.arachnid.find_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lesson 3 - Quiz: Using \\$match and \\$project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Write an aggregation query to answer this question:\n",
    "\n",
    "Of the users in the \"Brasilia\" timezone who have tweeted 100 times or more,\n",
    "who has the largest number of followers?\n",
    "\n",
    "The following hints will help you solve this problem:\n",
    "- Time zone is found in the \"time_zone\" field of the user object in each tweet.\n",
    "- The number of tweets for each user is found in the \"statuses_count\" field.\n",
    "  To access these fields you will need to use dot notation (from Lesson 4)\n",
    "- Your aggregation query should return something like the following:\n",
    "{u'ok': 1.0,\n",
    " u'result': [{u'_id': ObjectId('52fd2490bac3fa1975477702'),\n",
    "                  u'followers': 2597,\n",
    "                  u'screen_name': u'marbles',\n",
    "                  u'tweets': 12334}]}\n",
    "Note that you will need to create the fields 'followers', 'screen_name' and 'tweets'.\n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation \n",
    "pipeline that can be passed to the MongoDB aggregate function. As in our examples in this lesson,\n",
    "the aggregation pipeline should be a list of one or more dictionary objects. \n",
    "Please review the lesson examples if you are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you want to run this code\n",
    "locally on your machine, you have to install MongoDB, download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the twitter dataset used \n",
    "in examples in this lesson. If you attempt some of the same queries that we looked at in the lesson \n",
    "examples, your results will be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [\n",
    "        {\"$match\": {\"user.time_zone\": \"Brasilia\", \"user.statuses_count\": {\"$gte\": 100}}},\n",
    "        {\"$project\": {\"screen_name\": \"$user.screen_name\",\n",
    "                      \"followers\": \"$user.followers_count\",\n",
    "                      \"tweets\": \"$user.statuses_count\"}},\n",
    "        {\"$sort\": {\"followers\": -1}},\n",
    "        {\"$limit\": 1}\n",
    "    ]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.tweets.aggregate(pipeline)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import pprint\n",
    "    db = get_db('twitter')\n",
    "    pprint.pprint(db.tweets.find_one())\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    pprint.pprint(result)\n",
    "    assert len(result) == 1\n",
    "    assert result[0][\"followers\"] == 17209\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Using $unwind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "For this exercise, let's return to our cities infobox dataset. The question we would like you to answer\n",
    "is as follows:  Which region or district in India contains the most cities? (Make sure that the count of\n",
    "cities is stored in a field named 'count'; see the assertions at the end of the script.)\n",
    "\n",
    "As a starting point, use the solution for the example question we looked at -- \"Who includes the most\n",
    "user mentions in their tweets?\"\n",
    "\n",
    "One thing to note about the cities data is that the \"isPartOf\" field contains an array of regions or \n",
    "districts in which a given city is found. See the example document in Instructor Comments below.\n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation pipeline \n",
    "that can be passed to the MongoDB aggregate function. As in our examples in this lesson, the aggregation \n",
    "pipeline should be a list of one or more dictionary objects. Please review the lesson examples if you \n",
    "are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you want to run this code \n",
    "locally on your machine, you have to install MongoDB, download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the cities collection used in \n",
    "examples in this lesson. If you attempt some of the same queries that we looked at in the lesson \n",
    "examples, your results may be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [\n",
    "        {\"$match\": {\"country\": \"India\"}},\n",
    "        {\"$unwind\": \"$isPartOf\"},\n",
    "        {\"$group\": {\"_id\": \"$isPartOf\", \"count\": {\"$sum\": 1} }},\n",
    "        {\"$sort\": {\"count\": - 1}}\n",
    "    ]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.cities.aggregate(pipeline)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('examples')\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    print \"Printing the first result:\"\n",
    "    import pprint\n",
    "    pprint.pprint(result[0])\n",
    "    assert result[0][\"_id\"] == \"Uttar Pradesh\"\n",
    "    assert result[0][\"count\"] == 623\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Using $push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "$push is similar to $addToSet. The difference is that rather than accumulating only unique values \n",
    "it aggregates all values into an array.\n",
    "\n",
    "Using an aggregation query, count the number of tweets for each user. In the same $group stage, \n",
    "use $push to accumulate all the tweet texts for each user. Limit your output to the 5 users\n",
    "with the most tweets. \n",
    "Your result documents should include only the fields:\n",
    "\"_id\" (screen name of user), \n",
    "\"count\" (number of tweets found for the user),\n",
    "\"tweet_texts\" (a list of the tweet texts found for the user).  \n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation \n",
    "pipeline that can be passed to the MongoDB aggregate function. As in our examples in this lesson, \n",
    "the aggregation pipeline should be a list of one or more dictionary objects. \n",
    "Please review the lesson examples if you are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you want to run this code \n",
    "locally on your machine, you have to install MongoDB, download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the twitter dataset used in \n",
    "examples in this lesson. If you attempt some of the same queries that we looked at in the lesson \n",
    "examples, your results will be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [\n",
    "        # Group by \"screen_name\", collect \"tweet_texts\" in an array.\n",
    "        {\"$group\": {\"_id\": \"$user.screen_name\", \n",
    "                    \"tweet_texts\": {\"$push\": \"$text\"} }},\n",
    "        # Select the fields \"tweet_texts\" and \"count\" which is computed as the number of entries in \"tweet_texts\".\n",
    "        {\"$project\": {\"tweet_texts\": 1,\n",
    "                      \"count\": {\"$size\": \"$tweet_texts\"} }},\n",
    "        # Sort descending.\n",
    "        {\"$sort\": {\"count\": -1}},\n",
    "        # Select the first five elements.\n",
    "        {\"$limit\": 5},\n",
    "    ]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.twitter.aggregate(pipeline)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('twitter')\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    import pprint\n",
    "    pprint.pprint(result)\n",
    "    assert len(result) == 5\n",
    "    assert result[0][\"count\"] > result[4][\"count\"]\n",
    "    sample_tweet_text = u'Take my money! #liesguystell http://movie.sras2.ayorganes.com'\n",
    "    assert result[4][\"tweet_texts\"][0] == sample_tweet_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: //Title// TBD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "In an earlier exercise we looked at the cities dataset and asked which region in India contains \n",
    "the most cities. In this exercise, we'd like you to answer a related question regarding regions in \n",
    "India. What is the average city population for a region in India? Calculate your answer by first \n",
    "finding the average population of cities in each region and then by calculating the average of the \n",
    "regional averages.\n",
    "\n",
    "Hint: If you want to accumulate using values from all input documents to a group stage, you may use \n",
    "a constant as the value of the \"_id\" field. For example, \n",
    "    { \"$group\" : {\"_id\" : \"India Regional City Population Average\",\n",
    "      ... }\n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation \n",
    "pipeline that can be passed to the MongoDB aggregate function. As in our examples in this lesson, \n",
    "the aggregation pipeline should be a list of one or more dictionary objects. \n",
    "Please review the lesson examples if you are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you want to run this code \n",
    "locally on your machine, you have to install MongoDB, download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the twitter dataset used \n",
    "in examples in this lesson. If you attempt some of the same queries that we looked at in the lesson \n",
    "examples, your results will be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [\n",
    "        {\"$match\": {\"country\": \"India\"}},\n",
    "        # Creates one document for every entry in the \"isPartOf\" field, which enables aggregation later on.\n",
    "        {\"$unwind\": \"$isPartOf\"},\n",
    "        # Group entries with same isPartOf and average the population.\n",
    "        {\"$group\": {\"_id\": \"$isPartOf\", \"regional_avg\": {\"$avg\": \"$population\"}}},\n",
    "        # Group by a constant, which means \"group all\". Average the population.\n",
    "        {\"$group\": {\"_id\": \"India Regional City Population Average\", \"avg\": {\"$avg\": \"$regional_avg\"}}},\n",
    "    ]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.cities.aggregate(pipeline)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('examples')\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    import pprint\n",
    "    pprint.pprint(result)\n",
    "    assert len(result) == 1\n",
    "    # Your result should be close to the value after the minus sign.\n",
    "    assert abs(result[0][\"avg\"] - 201128.0241546919) < 10 ** -8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
