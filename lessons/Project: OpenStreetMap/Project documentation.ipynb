{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project documentation\n",
    "\n",
    "## Encountered problems and data cleaning\n",
    "\n",
    "The code for parsing nodes and ways is pretty close to what has been covered in the lectures. A few extensions have been necessary to extract some more meta information from the data:\n",
    "\n",
    " * Extract shops and shop specific information\n",
    " * Compute the length of streets\n",
    " * Restrict ways to actual streets\n",
    " * Extract street name pattern\n",
    " * Special characters\n",
    "\n",
    "Lets discuss these problems in more detail:\n",
    "\n",
    "### Extract shops and shop specific information\n",
    "For the following statistics the number of shops of the same name will be investigated. After parsing the data it became clear, that nodes which are shops have a specific set of tags. For all shops, these are read and restructured, so that the information becomes available under the new \"shop\" entry.\n",
    "\n",
    "The following code parses the tags of a node and restructures the data so that a dictionary from a set of selected _k_ to the corresponding values _v_ is created. Here we extract the name, wheelchair accessibility and opending hours for shops. But this can be easily extended to, say, bus stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Maps each type of special information to a list of fields which we care for.\n",
    "SPECIAL_TYPES = {\"shop\": [\"name\", \"wheelchair\", \"opening_hours\"]}\n",
    "\n",
    "    \n",
    "def parse_specials(element):\n",
    "    \"\"\"Parses and restructures special information for nodes.\"\"\"\n",
    "    content = {}\n",
    "    type_ = None\n",
    "    for subtag in element.iter(\"tag\"):\n",
    "        k = subtag.attrib[\"k\"]\n",
    "        v = subtag.attrib[\"v\"]\n",
    "        if k in SPECIAL_TYPES:\n",
    "            type_ = k\n",
    "        else:\n",
    "            content[k] = v\n",
    "    content = {k: v for k, v in content.items() \n",
    "               if type_ in SPECIAL_TYPES and k in SPECIAL_TYPES[type_]}       \n",
    "    \n",
    "    return type_, content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, this transforms the following input into the Python dictionary below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<node id=\"25433634\" lat=\"51.0195473\" lon=\"13.7223831\" version=\"8\" timestamp=\"2017-08-11T16:08:51Z\" changeset=\"51038779\" uid=\"86504\" user=\"Thomas8122\">\n",
    "    <tag k=\"level\" v=\"0\"/>\n",
    "    <tag k=\"name\" v=\"Kaufland\"/>\n",
    "    <tag k=\"opening_hours\" v=\"Mo-Sa 07:00-21:00\"/>\n",
    "    <tag k=\"shop\" v=\"supermarket\"/>\n",
    "    <tag k=\"wheelchair\" v=\"yes\"/>\n",
    "</node>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{\"shop\": {\"name\": \"Kaufland\", \"opening_hours\": \"Mo-Sa 07:00-21:00\", \"wheelchair\": \"yes\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the length of streets\n",
    "During the data wrangling the lenght of streets has to be computed, so that later on statistics on the length of streets can assembled. This is done in three steps:\n",
    "\n",
    "1. Remembering the coordinates of the nodes while parsing the `<node>` XML in a map `NODE_LOCATIONS`. \n",
    "2. When parsing the `<way>` XML we first collect the `ref` attribute of the `<nd>` tags and resolve them using the `NODE_LOCATIONS` map.\n",
    "3. Compute the pairwise distance between adjacent nodes and sum up over the course of the street. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stores node coordinates for distance calculation during node parsing.\n",
    "NODE_LOCATIONS = {}  \n",
    "\n",
    "\n",
    "def parse_way(element):\n",
    "    way = {}\n",
    "    element_counter[\"way\"] += 1\n",
    "    way_nodes = parse_way_nodes(element)\n",
    "    if way_nodes:\n",
    "        way[\"nodes\"] = way_nodes    \n",
    "        way[\"length\"] = compute_way_length(way_nodes)\n",
    "    return way\n",
    "\n",
    "\n",
    "def parse_way_nodes(element):\n",
    "    nodes = []\n",
    "    for node in element.iter(\"nd\"):\n",
    "        nodes.append(node.attrib[\"ref\"])\n",
    "    return nodes     \n",
    "\n",
    "\n",
    "def compute_way_length(way):\n",
    "    \"\"\"Assumes that the way nodes are in the correct order.\"\"\"\n",
    "    distance = 0\n",
    "    for s, t in pairwise(way):\n",
    "        try:\n",
    "            lat1, lon1 = NODE_LOCATIONS[s]\n",
    "            lat2, lon2 = NODE_LOCATIONS[t]\n",
    "            distance += geo_distance(lat1, lon1, lat2, lon2)\n",
    "        except:\n",
    "            pass  # ignore errors, which happens a lot in sampled data\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the Euclidean distance would yield a sufficient approximation for this part of the earth, it is generally not applicable for computing the distance between coordinates on the earth's surface. Especially when it comes to extreme latitudes (close to +-90°) the error becomes very high. Instead, we assume the world is a perfect ball sphere and use the great-circle distance.\n",
    "\n",
    "The functions for `geo_distance` and `pairwise` are skipped for brevity. They can be looked up in the file `audit.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on data size\n",
    "The dataset becomes too large during the computation because I need to store the coordinates of each node in order to compute the length of streets later on. In order to fit it into the 3GB RAM of my machine, I needed to drop other data like the create-information from OSM. I am aware that this could have been done on MongoDB as well, however, the Python API falls back to use JavaScript in its MapReduce code and I am not fluent with that, so it seemed more reasonable to do this within the parser code.\n",
    "\n",
    "The following code skips this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CREATED = [\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "\n",
    "def parse_element(element):\n",
    "    entity = None\n",
    "    if element.tag == \"way\":\n",
    "        entity = parse_way(element)\n",
    "    elif element.tag == \"node\":\n",
    "        entity = parse_node(element)\n",
    "    entity[\"type\"] = element.tag\n",
    "    lat = lon = None\n",
    "    for k, v in element.attrib.items():\n",
    "        if k in CREATED:\n",
    "            # ignore the creation data\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict ways to actual streets\n",
    "The investigation should only focus on actual streets. There are extracted by focusing on `<way>` elements with the tag `highway` and a valid `name` tag. There are some strange street names. So I created a list of street name patterns (like \"Straße\" or \"Ring\" or more descriptive types of street names like \"An der Fabrik\" which is \"Next to the factory\") and iteratively extended the list until the pattern matched all the valid street names. All other streets and ways which are not streets (i.e. do not have a \"highway\" tag) are discarded during the parsing. \n",
    "\n",
    "The following code checks if a common component is contained in the street name or if it starts with one of the typical patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_element(element):\n",
    "    # ...\n",
    "    \n",
    "    # For ways, only consider streets (highway tag) with a valid name:\n",
    "    if element.tag == \"way\":\n",
    "        tags = entity[\"tags\"]\n",
    "        if (\"highway\" in tags and \n",
    "            \"name\" in tags and \n",
    "            is_valid_street_name(tags[\"name\"])):\n",
    "            return entity\n",
    "        \n",
    "        \n",
    "common_street_names = re.compile(\n",
    "        r\"(weg|straße|platz|allee|gasse|ring|berg|grund|\" +\n",
    "        r\"steig|hof|ufer|höhe|leite|brücke|passage|steg|graben|tunnel)\", \n",
    "        re.IGNORECASE)\n",
    "common_start_phrases = re.compile(\n",
    "        r\"(Am |An de|Alt|Hinter de|Im |Zum |Zur )\", \n",
    "        re.IGNORECASE)\n",
    "\n",
    "\n",
    "def is_valid_street_name(name):\n",
    "    return common_street_names.search(name) or common_start_phrases.match(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract street name pattern\n",
    "This came to my mind when restricting to actual streets. If we already check for common street name components, why not extract them and create a statistic on which are the most common? \n",
    "\n",
    "The following code reuses the patterns created above and stores the matching regular expression groups within the way data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_street_name_component(name):\n",
    "    match = common_street_names.search(name)\n",
    "    if match:\n",
    "        return match.group(1).lower()\n",
    "    match = common_start_phrases.match(name)\n",
    "    if match:\n",
    "        return match.group(1).lower()\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example how this transforms XML into a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<way id=\"25359184\" version=\"5\" timestamp=\"2014-08-24T15:57:51Z\" changeset=\"24982129\" uid=\"48393\" user=\"Wurgwitz\">\n",
    "    <nd ref=\"2729352943\"/>\n",
    "    <nd ref=\"3039521232\"/>\n",
    "    <nd ref=\"3039521213\"/>\n",
    "    <tag k=\"name\" v=\"Hinter dem Rathaus\"/>\n",
    "    <tag k=\"highway\" v=\"residential\"/>\n",
    "</way>\n",
    "<way id=\"25368605\" version=\"3\" timestamp=\"2014-07-05T11:47:01Z\" changeset=\"23965179\" uid=\"2675\" user=\"Eckhart Wörner\">\n",
    "    <nd ref=\"35314674\"/>\n",
    "    <nd ref=\"578090458\"/>\n",
    "    <nd ref=\"276516857\"/>\n",
    "    <tag k=\"name\" v=\"Hirschbacher Weg\"/>\n",
    "    <tag k=\"highway\" v=\"residential\"/>\n",
    "    <tag k=\"maxspeed\" v=\"30\"/>\n",
    "    <tag k=\"postal_code\" v=\"01277\"/>\n",
    "</way>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{\"name\": \"Hinter dem Rathaus\", \"street_name_component_match\": \"hinter de\"},\n",
    "{\"name\": \"Hirschbacher Weg\", \"street_name_component_match\": \"weg\"},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special characters (UTF-8 handling)\n",
    "The dataset is from Germany and thus many names includes German special characters, encoded as UTF-8. I expected a lot more trouble with handling these correctly. However, it turned out that Python3 completely gets the encoding right and there have been no problems with special characters at all, even when transferring the data to MongoDB and back. The key is to used Python 3 instead of Python 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the data\n",
    "\n",
    "### General statistics\n",
    "\n",
    "The following numbers have been determined from the database. Note that during parsing, some irrelevant data has been dropped for workaround memory issuues. For instance, the following investigation does not care about ways which are not roads (tagged as \"highway\") and the information which OSM user created the elements at what time etc. pp.\n",
    "\n",
    "#### File size\n",
    "\n",
    "The uncompressed dataset has a size of 312 MByte.\n",
    "\n",
    "#### Number of elements\n",
    "`> db.osm_dresden.count()`\n",
    "\n",
    "1341909\n",
    "\n",
    "\n",
    "#### Number of nodes\n",
    "`> db.osm_dresden.find({\"type\": \"node\"}).count()`\n",
    "\n",
    "1323207\n",
    "\n",
    "#### Number of streets\n",
    "`> db.osm_dresden.find({\"type\": \"way\"}).count()`\n",
    "\n",
    "18702\n",
    "\n",
    "\n",
    "### Computed statistics\n",
    "\n",
    "#### Most frequent shops in town\n",
    "\n",
    "| Name | Count |\n",
    "|-|-|\n",
    "|None|340|\n",
    "|Rossmann|19|\n",
    "|Konsum|17|\n",
    "|Netto Marken-Discount|13|\n",
    "|Netto|13|\n",
    "|Fleischerei Richter|12|\n",
    "|Rewe|11|\n",
    "|Richter|11|\n",
    "|Sternenbäck|11|\n",
    "|Eisold|9|\n",
    "\n",
    "#### Longest streets:\n",
    "\n",
    "| Street name | lenth in kilometers |\n",
    "|-|-|\n",
    "|Ullersdorf-Langebrücker Straße|2.49|\n",
    "|Radeberger Landstraße|2.49|\n",
    "|Fütterungsweg|2.44|\n",
    "|Mittelweg|2.35|\n",
    "|Tunnel Coschütz|2.35|\n",
    "|Tunnel Coschütz|2.31|\n",
    "|Alter Bahndamm|2.29|\n",
    "|Ullersdorf-Langebrücker Straße (5)|2.18|\n",
    "|Prießnitzgrundweg|2.09|\n",
    "|Kesselsdorfer Straße|1.98|\n",
    "\n",
    "#### Streets with the most nodes:\n",
    "\n",
    "| Street name | number of nodes |\n",
    "|-|-|\n",
    "|Schloßplatz|120|\n",
    "|Am Pulverturm|75|\n",
    "|Leitenweg|74|\n",
    "|Wiener Straße|74|\n",
    "|Seebachstraße|70|\n",
    "|Wieckestraße|68|\n",
    "|Jorge-Gomondai-Platz|67|\n",
    "|Unkersdorfer Landstraße|67|\n",
    "|Altgorbitzer Ring|66|\n",
    "|Elberadweg|63|\n",
    "\n",
    "#### Frequency of common street name components:\n",
    "\n",
    "| Street name component | absolute frequency |\n",
    "|-|-|\n",
    "|straße|10148|\n",
    "|weg|2719|\n",
    "|berg|1318|\n",
    "|platz|690|\n",
    "|am |631|\n",
    "|hof|459|\n",
    "|alt|392|\n",
    "|grund|316|\n",
    "|brücke|311|\n",
    "|an de|297|\n",
    "|ring|297|\n",
    "|allee|245|\n",
    "|steig|144|\n",
    "|gasse|141|\n",
    "|ufer|124|\n",
    "|höhe|117|\n",
    "|zur |93|\n",
    "|leite|70|\n",
    "|graben|65|\n",
    "|zum |64|\n",
    "|steg|34|\n",
    "|tunnel|12|\n",
    "|im |7|\n",
    "|passage|5|\n",
    "|hinter de|3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ideas about the dataset\n",
    "\n",
    "During the project, the following extensions of the analysis came to my mind:\n",
    "\n",
    "* Determine and combine split streets\n",
    "* Missing shop names\n",
    "* Parse and process shop opening hours\n",
    "* More precise distance function\n",
    "\n",
    "Lets discuss them in some detail.\n",
    "\n",
    "### Determine and combine split streets\n",
    "\n",
    "Some long streets seem to be split in the data set: There are streets which have the same name or the same name except for some number. For some reason, these are separated in the OSM data.\n",
    "\n",
    "One could determine these streets with similar names, combine them and then compute the length of the combined streets. However, also distinct streets may come with the same name. These should be merged. Finding out which street parts belong together and which are distinct is not trivial.\n",
    "\n",
    "\n",
    "### Missing shop names\n",
    "\n",
    "Many shops do not have a name. This cannot be reconstructed from the data. I decided to keep them in the statistic anyway. It would be interesting to find out why there is no name, for example by checking some of the shops on Google street view or reconstructing them by looking up the addresses in an external API.\n",
    "\n",
    "\n",
    "### Parse and process shop opening hours\n",
    "\n",
    "In the present data auditing, shop opening hours have just been read as-is. This means that most are composed of German short terms for weekdays and hours of the day. This should really be converted to datetime objects. The results could be stored in the shop data, and a MongoDB aggregation query could be constructed that yields the number of open grocery stores for every hour of the day. The query would select all shops from the data, gorup them by opening hours and then count the number of shops per hour of the day. \n",
    "\n",
    "The query could look like this (untested):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = db.osm_dresden.aggregate([\n",
    "    {\"$match\": {\"type\": {\"$eq\": \"node\"}, \"$exists\": {\"shop\": True}}},  # select only shops\n",
    "    {\"$unwind\": \"$opening_hours\"},  # create one entry per each opening hour\n",
    "    {\"$group\": {\"_id\": \"$opening_hours\", \"count\": {\"$sum\": 1}}},  # group by opening hour and count number of open shops\n",
    "    {\"$sort\": {\"_id\": 1}},  # sort by field opening hour, ascending\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More precise distance function\n",
    "\n",
    "The great-circle distance used to compute the street length assumes the earth is a perfect sphere, which is not. to get more precise values, the [WGS 84](https://en.wikipedia.org/wiki/World_Geodetic_System#WGS84) projection could be used which models the earth as an ellipsoid.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "After doing some analysis and restructuring of the data, it seems that the selected data set is of fairly good quality. German special characters are encoded in a sound way. The information on different entities is surprisingly detailed. There are some streets which are separated in the data where no reason or pattern could be found. \n",
    "\n",
    "The present project created analysed the longest streets in the data set, listed the most frequent shops in the area and did a lexicographical analysis on the frequency of common street name wordings. \n",
    "\n",
    "Several extensions have been suggested which could be used to draw conclusions from the data.\n",
    "\n",
    "\n",
    "### A note about sampling\n",
    "\n",
    "The project submission includes only a sample of the data which has been created using the code provided in the Project guideline. The calculation of the street lenght works only correctly if every node of a street is given. The sampling process leads to OSM ways beeing fragmented. Thus the resulting lengths are probably not correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
