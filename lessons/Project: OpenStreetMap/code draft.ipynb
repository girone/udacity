{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an excerpt of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See create_sample.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Count element types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from collections import defaultdict\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "\n",
    "def count_tags(filename):\n",
    "    counter = defaultdict(int)\n",
    "    with open(filename) as f:\n",
    "        for event, element in ET.iterparse(f):\n",
    "            counter[element.tag] += 1\n",
    "    return counter\n",
    "    \n",
    "\n",
    "def test():\n",
    "    tags = count_tags('dresden_germany.sample_k=100.osm')\n",
    "    pprint.pprint(tags)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse node and ways with their tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote to dresden_germany.sample_k=100.osm.json\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import xml.etree.cElementTree as ET\n",
    "import re\n",
    "import codecs\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "\n",
    "filename = 'dresden_germany.sample_k=100.osm'\n",
    "#filename = 'dresden_germany.sample_k=10.osm'\n",
    "#filename = 'dresden_germany.osm'\n",
    "\n",
    "TOP_LEVEL_TAGS = [\"way\", \"node\"]\n",
    "CREATED = [\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "# Stores node coordinates for distance calculation.\n",
    "NODE_LOCATIONS = {}\n",
    "\n",
    "\n",
    "# Count tag keys per parent element type (e.g. \"node\", \"way\", ...).\n",
    "tag_keys = defaultdict(Counter)\n",
    "element_counter = Counter()\n",
    "\n",
    "\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    from itertools import tee\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "\n",
    "def geo_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Compute the distance between two points on the earth given by latitude and longitude.\n",
    "    \n",
    "    Courtes of https://stackoverflow.com/a/19412565/841567.\n",
    "    \"\"\"\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(lon1)\n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(lon2)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "\n",
    "def compute_way_length(way):\n",
    "    \"\"\"Assumes that the way nodes are in the correct order.\"\"\"\n",
    "    distance = 0\n",
    "    for s, t in pairwise(way):\n",
    "        try:\n",
    "            lat1, lon1 = NODE_LOCATIONS[s]\n",
    "            lat2, lon2 = NODE_LOCATIONS[t]\n",
    "            distance += geo_distance(lat1, lon1, lat2, lon2)\n",
    "        except:\n",
    "            pass  # ignore errors, which happens a lot in sampled data\n",
    "    return distance\n",
    "\n",
    "    \n",
    "def parse_address(element):\n",
    "    address = {}\n",
    "    for subtag in element.iter(\"tag\"):\n",
    "        k = subtag.attrib[\"k\"]\n",
    "        v = subtag.attrib[\"v\"]\n",
    "        if problemchars.match(k):\n",
    "            print(k)\n",
    "            continue\n",
    "        elif k.count(\":\") > 1:\n",
    "            continue\n",
    "        elif k.startswith(\"addr:\"):\n",
    "            key = k.split(\":\")[1]\n",
    "            address[key] = v\n",
    "    return address\n",
    "        \n",
    "    \n",
    "# Maps each type of special information to a list of fields which we care for.\n",
    "SPECIAL_TYPES = {\"shop\": [\"name\", \"wheelchair\", \"opening_hours\"]}\n",
    "\n",
    "    \n",
    "def parse_specials(element):\n",
    "    \"\"\"Parses and restructures special information for nodes.\"\"\"\n",
    "    content = {}\n",
    "    type_ = None\n",
    "    for subtag in element.iter(\"tag\"):\n",
    "        k = subtag.attrib[\"k\"]\n",
    "        v = subtag.attrib[\"v\"]\n",
    "        if k in SPECIAL_TYPES:\n",
    "            type_ = k\n",
    "        else:\n",
    "            content[k] = v\n",
    "    content = {k: v for k, v in content.items() \n",
    "               if type_ in SPECIAL_TYPES and k in SPECIAL_TYPES[type_]}       \n",
    "    \n",
    "    return type_, content\n",
    "    \n",
    "    \n",
    "def parse_node(element):\n",
    "    node = {}\n",
    "    element_counter[\"node\"] += 1\n",
    "    address = parse_address(element)\n",
    "    if address:\n",
    "        node[\"address\"] = address\n",
    "    special_type, special_content = parse_specials(element)\n",
    "    if special_type:\n",
    "        node[special_type] = special_content\n",
    "    return node\n",
    "\n",
    "\n",
    "def parse_way_nodes(element):\n",
    "    nodes = []\n",
    "    for node in element.iter(\"nd\"):\n",
    "        nodes.append(node.attrib[\"ref\"])\n",
    "    return nodes     \n",
    "\n",
    "\n",
    "def parse_way(element):\n",
    "    way = {}\n",
    "    element_counter[\"way\"] += 1\n",
    "    way_nodes = parse_way_nodes(element)\n",
    "    if way_nodes:\n",
    "        way[\"nodes\"] = way_nodes    \n",
    "        way[\"length\"] = compute_way_length(way_nodes)\n",
    "    return way\n",
    "\n",
    "\n",
    "def parse_tags(element):\n",
    "    tags = {}\n",
    "    for tag in element.iter(\"tag\"):\n",
    "        #print(\"tag\", tag.attrib)\n",
    "        k, v = tag.attrib[\"k\"], tag.attrib[\"v\"]\n",
    "        if k.startswith(\"addr:\"):\n",
    "            continue  # skip address tags, will be parsed otherwise\n",
    "        tags[k] = v\n",
    "    return tags\n",
    "\n",
    "\n",
    "common_street_names = re.compile(\n",
    "        r\"(weg|straße|platz|allee|gasse|ring|berg|grund|\" +\n",
    "        r\"steig|hof|ufer|höhe|leite|brücke|passage|steg|graben|tunnel)\", \n",
    "        re.IGNORECASE)\n",
    "common_start_phrases = re.compile(\n",
    "        r\"(Am |An de|Alt|Hinter de|Im |Zum |Zur )\", \n",
    "        re.IGNORECASE)\n",
    "\n",
    "\n",
    "def is_valid_street_name(name):\n",
    "    return common_street_names.search(name) or common_start_phrases.match(name)\n",
    "\n",
    "\n",
    "def extract_street_name_component(name):\n",
    "    match = common_street_names.search(name)\n",
    "    if match:\n",
    "        return match.group(1).lower()\n",
    "    match = common_start_phrases.match(name)\n",
    "    if match:\n",
    "        return match.group(1).lower()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def parse_element(element):\n",
    "    entity = None\n",
    "    if element.tag == \"way\":\n",
    "        entity = parse_way(element)\n",
    "    elif element.tag == \"node\":\n",
    "        entity = parse_node(element)\n",
    "    entity[\"created\"] = {}\n",
    "    entity[\"type\"] = element.tag\n",
    "    lat = lon = None\n",
    "    for k, v in element.attrib.items():\n",
    "        if k in CREATED:\n",
    "            # ignore the creation data\n",
    "            pass\n",
    "        elif k == \"lat\":\n",
    "            lat = float(v)\n",
    "        elif k == \"lon\":\n",
    "            lon = float(v)\n",
    "        else:\n",
    "            entity[k] = v\n",
    "    if lat and lon:\n",
    "        entity[\"pos\"] = [lat, lon]\n",
    "        NODE_LOCATIONS[entity[\"id\"]] = [lat, lon]\n",
    "        \n",
    "    # Parse the tags.\n",
    "    tags = parse_tags(element)\n",
    "    entity[\"tags\"] = tags if tags else {}\n",
    "\n",
    "    # For ways, only consider streets (highway tag) with a valid name:\n",
    "    if element.tag == \"way\":\n",
    "        tags = entity[\"tags\"]\n",
    "        if (\"highway\" in tags and \n",
    "            \"name\" in tags and \n",
    "            is_valid_street_name(tags[\"name\"])):\n",
    "            string = extract_street_name_component(tags[\"name\"])\n",
    "            entity[\"street_name_component_match\"] = string\n",
    "            return entity\n",
    "        else:\n",
    "            return None\n",
    "    return entity\n",
    "\n",
    "    \n",
    "filename_out = \"{0}.json\".format(filename)\n",
    "with open(filename) as f:\n",
    "    with codecs.open(filename_out, \"w\") as fout:\n",
    "        for event, element in ET.iterparse(f, events=[\"end\"]):\n",
    "            if element.tag in TOP_LEVEL_TAGS:\n",
    "                #print(element)\n",
    "                el = parse_element(element)\n",
    "                if el:\n",
    "                    #fout.write(json.dumps(el, indent=2) + \"\\n\")\n",
    "                    fout.write(json.dumps(el) + \"\\n\")\n",
    "        print(\"Wrote to {}\".format(filename_out))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "  * Parse the data using cElementTree.\n",
    "  * Create some statistics while parsing, document them. Note: Maybe this should be done in the end, using queries against MongoDB.\n",
    "  * Restrict the ways in the dataset to actual streets (ways with a tag with k=highway) (drops shapes of buildings) (/)\n",
    "  * Do a little data cleaning:\n",
    "    * Parse street name tags and unify abbreviations (later include the types into the statistics) (/)\n",
    "    * Drop \"created\" content. (/)\n",
    "    * Unicode works fine after importing to MongoDB? (/) -- Just use Python3.\n",
    "  * Create JSON output from the data  (/)\n",
    "  * Import it into MongoDB (/)\n",
    "    * mongoimport -d udacity_dand -c osm_dresden dresden_germany.sample_k\\=10.osm.json \n",
    "  * Run statistics queries against it, audit, find and document problems, iterate from the beginning\n",
    "    * Most frequent shop name (/)\n",
    "    * Most frequent street type (name component)\n",
    "    * Number of ways, nodes\n",
    "    * Longest way (most way_nodes, longest distance) (/)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encountered problems (and solution)\n",
    "\n",
    "  * Many elements are duplicated -- by my mistake or due to errorneous data?\n",
    "    * It was my mistake, caused by reading \"end\" and \"start\" events from iterparse.\n",
    "  * mongoimport reads one element per line, not prettyfied JSON\n",
    "  * fix UTF-8 encoding / decoding\n",
    "  * What is the most frequent shop? Some nodes have a shop field, but no name. Find out what is going on and if these can be safely ignored.\n",
    " \n",
    "## Audited data\n",
    "\n",
    "  * shops: parse name, wheelchair accessibility and opening hours\n",
    "  * places: detect suburbs etc. (todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mongo shell\n",
    "\n",
    "Various commands:\n",
    "* Delete a database: mongo database_name --eval \"db.dropDatabase()\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
